# Palimpsest

**A personal journal metadata management and PDF compilation system.**

[![Tests](https://github.com/soffiafdz/palimpsest/actions/workflows/test.yml/badge.svg)](https://github.com/soffiafdz/palimpsest/actions/workflows/test.yml)
[![Integration Tests](https://github.com/soffiafdz/palimpsest/actions/workflows/integration.yml/badge.svg)](https://github.com/soffiafdz/palimpsest/actions/workflows/integration.yml)
[![Security](https://github.com/soffiafdz/palimpsest/actions/workflows/security.yml/badge.svg)](https://github.com/soffiafdz/palimpsest/actions/workflows/security.yml)
[![codecov](https://codecov.io/gh/soffiafdz/palimpsest/branch/main/graph/badge.svg)](https://codecov.io/gh/soffiafdz/palimpsest)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Code style: Ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)

Palimpsest is a personal project consisting of a Python-based toolkit for processing, organizing, and analyzing journal entries with rich metadata. It converts raw text exports into structured Markdown files with YAML frontmatter, maintains a SQLite database of relationships and themes, and generates annotated PDFs for review and curation.

Originally built for managing my decade+ archive from [750words.com](https://750words.com), Palimpsest provides me the infrastructure for transforming my personal documentary writings into searchable, cross-referenced material suitable for memoir or creative non-fiction projects.

---

## Features

- **Multi-stage processing pipeline**: Raw exports â†’ Formatted text â†’ Markdown â†’ Database â†’ Wiki â†’ PDFs
- **Rich metadata extraction**: Track people, locations, events, themes, dates, and references
- **Database-backed queries**: SQLAlchemy ORM with relationship mapping and analytics
- **Wiki system**: Bidirectional sync between database and Markdown wiki for editing and curation
- **Full-text search**: SQLite FTS5 with advanced filtering (people, dates, themes, word count, etc.)
- **Automated metadata extraction** (optional): NLP-based tagging, named entity recognition, and pattern matching for organizational purposes
- **Manuscript subwiki**: Dedicated wiki for curating journal entries into literary material
- **PDF generation**: Create clean reading copies and annotated review versions
- **Vim/Neovim integration**: Vimwiki templates and automation (optional)
- **Makefile orchestration**: Simple commands for batch processing and year-based builds

---

## ğŸ“ About Automated Analysis Tools

**The optional text analysis tools in this project (`dev/nlp/` and `nlp` command) use standard computational linguistics techniques**â€”specifically named entity recognition (NER), keyword extraction, and pattern matching to assist with metadata organization of existing journal content.

**Technical approach:**
- Uses spaCy for named entity recognition (local, open-source)
- Employs regex and keyword matching for theme detection
- Optional: sentence transformers for semantic similarity search
- Optional: LLM APIs for structured metadata extraction (Level 4 only)

**Important clarifications:**
- âœ… Extracts entities, themes, and tags from already-written text
- âœ… Suggests organizational structures based on text analysis
- âœ… Assists with cataloging and searchability through pattern matching
- âŒ **Does NOT generate, write, or modify any creative content**
- âŒ **Does NOT contribute to manuscript writing**
- âŒ **All journal entries and literary work are 100% human-written**

These are standard text processing techniques used in digital humanities, archives, and library science for organizing large text corpora.

---

## Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/soffiafdz/palimpsest.git
cd palimpsest

# Create conda/micromamba environment
micromamba env create -f environment.yaml
micromamba activate palimpsest

# Set up data directory (private submodule or local)
# Option 1: With private submodule
git submodule update --init --recursive

# Option 2: Create local data directory
mkdir -p data/journal/{inbox,sources/txt,content/{md,pdf}}
mkdir -p data/metadata

# Initialize database
metadb init
```

### Basic Usage

```bash
# Process new journal exports
plm inbox

# Convert to markdown
plm convert

# Sync database
plm sync-db

# Build PDFs for a year
plm build-pdf 2024

# Or run complete pipeline
plm run-all 2024
```

### Using Make

```bash
# Process everything
make all

# Year-specific
make 2024
make 2024-md   # Markdown only
make 2024-pdf  # PDFs only

# Database operations
make init-db
make backup
make stats
```

---

## Pipeline Architecture

```
inbox/ (raw exports) â†’ src2txt â†’ txt/     (formatted text)
                                   â†“
                                 txt2md
                                   â†“
                                 md/      (markdown + YAML)
                                   â†“            â†‘
                                yaml2sql   sql2yaml
                                   â†“            â†‘
                                database (SQLite + metadata + FTS5)
                                   â†“            â†‘
                                sql2wiki   wiki2sql
                                   â†“            â†‘
                                 wiki/    (editable wiki pages)
                                   â”œâ”€â”€ entries/
                                   â”œâ”€â”€ people/
                                   â”œâ”€â”€ events/
                                   â””â”€â”€ manuscript/

                                 md2pdf
                                   â†“
                                 pdf/     (annotated PDFs)
```

### Pipeline Scripts (X2Y Pattern)

Each pipeline step is implemented as a standalone script with both CLI and programmatic API:

- **`src2txt.py`**: Process raw 750words exports â†’ formatted text files
  - Validates filenames, runs format script, archives originals
  - Archives to: `data/journal/sources/archive/`

- **`txt2md.py`**: Convert formatted text â†’ Markdown with YAML frontmatter
  - Parses entries, computes metadata (word count, reading time)

- **`yaml2sql.py`**: Sync Markdown YAML â†’ Database
  - Parses complex metadata, manages relationships

- **`sql2yaml.py`**: Export Database â†’ Markdown YAML
  - Regenerates frontmatter while preserving body content

- **`md2pdf.py`**: Build Markdown â†’ PDFs (clean & notes versions)
  - Uses Pandoc + LaTeX for professional typography

- **Wiki scripts**: Bidirectional sync between database and wiki
  - `sql2wiki.py`: Database â†’ Wiki pages
  - `wiki2sql.py`: Wiki edits â†’ Database

### Data Flow Paths

1. **Journal â†’ Database**: `inbox â†’ txt â†’ md â†’ database` (via YAML frontmatter)
2. **Database â†’ Wiki**: `database â†’ wiki` (for editing and curation)
3. **Wiki â†’ Database**: `wiki â†’ database` (import edits back)
4. **Search & Analysis**: Query database with FTS5, extract metadata with NLP tools
5. **Export**: `database â†’ pdf` (annotated reading copies)

---

## Directory Structure

```
palimpsest/
â”œâ”€â”€ dev/                        # Source code
â”‚   â”œâ”€â”€ nlp/                    # NLP analysis (optional)
â”‚   â”‚   â”œâ”€â”€ extractors.py      # spaCy NER, theme extraction
â”‚   â”‚   â”œâ”€â”€ semantic_search.py # Sentence transformers
â”‚   â”‚   â”œâ”€â”€ claude_assistant.py # Claude API integration
â”‚   â”‚   â””â”€â”€ openai_assistant.py # OpenAI API integration
â”‚   â”œâ”€â”€ bin/                    # CLI wrappers (journal, metadb)
â”‚   â”œâ”€â”€ builders/               # PDF and text builders
â”‚   â”œâ”€â”€ core/                   # Logging, validation, paths
â”‚   â”œâ”€â”€ database/               # SQLAlchemy ORM and managers
â”‚   â”‚   â”œâ”€â”€ models.py          # Main database models
â”‚   â”‚   â”œâ”€â”€ models_manuscript.py # Manuscript models
â”‚   â”‚   â”œâ”€â”€ search.py          # Search query engine
â”‚   â”‚   â””â”€â”€ search_index.py    # FTS5 index manager
â”‚   â”œâ”€â”€ dataclasses/            # Entry data structures
â”‚   â”‚   â”œâ”€â”€ md_entry.py        # YAML frontmatter
â”‚   â”‚   â”œâ”€â”€ wiki_*.py          # Wiki page classes
â”‚   â”‚   â””â”€â”€ manuscript_*.py    # Manuscript wiki classes
â”‚   â”œâ”€â”€ pipeline/               # Processing scripts
â”‚   â”‚   â”œâ”€â”€ yaml2sql.py        # YAML â†’ Database
â”‚   â”‚   â”œâ”€â”€ sql2yaml.py        # Database â†’ YAML
â”‚   â”‚   â”œâ”€â”€ sql2wiki.py        # Database â†’ Wiki
â”‚   â”‚   â”œâ”€â”€ wiki2sql.py        # Wiki â†’ Database
â”‚   â”‚   â”œâ”€â”€ search.py          # Search CLI
â”‚   â”‚   â””â”€â”€ nlp_assist.py      # NLP analysis CLI
â”‚   â””â”€â”€ utils/                  # Utilities (fs, md, parsers)
â”œâ”€â”€ templates/                  # LaTeX preambles, wiki templates
â”œâ”€â”€ tests/                      # Integration tests
â”‚   â””â”€â”€ integration/
â”‚       â”œâ”€â”€ test_search.py     # Search tests
â”‚       â”œâ”€â”€ test_nlp_extraction.py # NLP tests
â”‚       â”œâ”€â”€ test_sql_to_wiki.py # Wiki export tests
â”‚       â””â”€â”€ test_wiki_to_sql.py # Wiki import tests
â”œâ”€â”€ data/                       # Personal content (git submodule)
â”‚   â”œâ”€â”€ journal/
â”‚   â”‚   â”œâ”€â”€ inbox/
â”‚   â”‚   â”œâ”€â”€ sources/txt/
â”‚   â”‚   â”œâ”€â”€ content/md/
â”‚   â”‚   â””â”€â”€ annotations/
â”‚   â”œâ”€â”€ manuscript/
â”‚   â”œâ”€â”€ wiki/                   # Generated wiki
â”‚   â”‚   â”œâ”€â”€ entries/
â”‚   â”‚   â”œâ”€â”€ people/
â”‚   â”‚   â”œâ”€â”€ events/
â”‚   â”‚   â”œâ”€â”€ cities/
â”‚   â”‚   â””â”€â”€ manuscript/
â”‚   â””â”€â”€ metadata/
â”‚       â””â”€â”€ palimpsest.db
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ BIDIRECTIONAL_SYNC_GUIDE.md # Wiki sync documentation
â”œâ”€â”€ environment.yaml
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

---

## Command Reference

### Pipeline Commands

```bash
# Process inbox (raw exports â†’ formatted text)
plm inbox [-i INBOX] [-o OUTPUT]

# Convert text to markdown
plm convert [-i INPUT] [-o OUTPUT] [-f]

# Sync database from markdown
plm sync-db [-i INPUT] [-f]

# Export database to markdown
plm export-db [-o OUTPUT] [-f]

# Build PDFs
plm build-pdf YEAR [-i INPUT] [-o OUTPUT] [-f]

# Complete pipeline
plm run-all [--year YEAR] [--skip-inbox] [--skip-pdf]

# Status
plm status
plm validate
```

### Database Commands

```bash
# Initialize
metadb init [--alembic-only] [--db-only]
metadb reset [--keep-backups]

# Migrations
metadb migration-create MESSAGE [--autogenerate]
metadb migration-upgrade [--revision REV]
metadb migration-status
metadb migration-history

# Backups
metadb backup [--type TYPE] [--suffix SUFFIX]
metadb backups
metadb restore BACKUP_PATH

# Monitoring
metadb stats [--verbose]
metadb health [--fix]
metadb validate

# Maintenance
metadb cleanup
metadb optimize

# Export
metadb export-csv OUTPUT_DIR
metadb export-json OUTPUT_FILE
metadb analyze
```

### Search Commands

```bash
# Text search with filters
jsearch "query text" [filters]
jsearch "therapy" person:alice in:2024 words:100-500
jsearch "reflection" city:montreal has:manuscript

# Index management
jsearch index --create
jsearch index --rebuild
jsearch index --status

# Available filters:
# person:NAME, tag:TAG, event:EVENT, city:CITY, theme:THEME
# in:YEAR, year:YEAR, month:MONTH
# from:DATE, to:DATE
# words:MIN-MAX, time:MIN-MAX
# has:manuscript, status:STATUS
# sort:relevance|date|word_count, limit:N
```

### Text Analysis Commands

```bash
# Check analysis capabilities
nlp status

# Analyze single entry (Level 2: spaCy NER)
nlp analyze 2024-11-01 --level 2

# Analyze with LLM API (Level 4)
nlp analyze 2024-11-01 --level 4 --manuscript

# Batch analyze entries
nlp batch --level 2 --limit 10

# Find semantically similar entries (Level 3)
nlp similar 2024-11-01 --limit 10

# Cluster entries by theme
nlp cluster --num-clusters 10
```

**Analysis Processing Levels:**

- **Level 2**: spaCy NER (free) - Entity extraction, theme detection
- **Level 3**: Sentence Transformers (free) - Semantic similarity search
- **Level 4**: LLM APIs (paid) - Advanced analysis, manuscript curation (Claude or OpenAI)

See [Search & Analysis Documentation](#search--analysis-features) for details.

---

## Wiki System

The wiki provides an editable interface for curating and annotating your journal:

### Export Database to Wiki

```bash
# Export everything
plm export-wiki all

# Export specific entity types
plm export-wiki people
plm export-wiki entries
plm export-wiki manuscript

# Force overwrite existing files
plm export-wiki all --force
```

### Import Wiki Edits to Database

```bash
# Import all wiki edits
plm import-wiki all

# Import specific entity type
plm import-wiki people
plm import-wiki entries
plm import-wiki manuscript
```

### Wiki Structure

```
wiki/
â”œâ”€â”€ index.md                    # Main index
â”œâ”€â”€ people/
â”‚   â”œâ”€â”€ index.md               # People index
â”‚   â””â”€â”€ alice.md               # Person page (editable notes)
â”œâ”€â”€ entries/
â”‚   â”œâ”€â”€ index.md               # Entries index
â”‚   â””â”€â”€ 2024-11-01.md          # Entry page (editable notes)
â”œâ”€â”€ events/
â”‚   â””â”€â”€ therapy-session.md     # Event page
â”œâ”€â”€ cities/
â”‚   â””â”€â”€ montreal.md            # City page
â””â”€â”€ manuscript/
    â”œâ”€â”€ index.md               # Manuscript index
    â”œâ”€â”€ entries/
    â”‚   â””â”€â”€ 2024-11-01.md      # Manuscript entry (adaptation notes)
    â””â”€â”€ characters/
        â””â”€â”€ alexandra.md        # Character page (voice, arc, description)
```

### Editable Fields

**Main Wiki** (only `notes` fields are editable):

- Person pages: Add biographical notes, relationship context
- Entry pages: Add editorial notes, manuscript potential
- Event/City pages: Add context notes

**Manuscript Wiki** (detailed curation fields):

- Manuscript entries: Entry type, narrative arc, character notes, adaptation notes
- Characters: Character description, arc, voice notes, appearance notes
- Themes, arcs, and other manuscript-specific metadata

See [docs/bidirectional-sync-guide.md](docs/bidirectional-sync-guide.md) for complete documentation.

---

## Search & Analysis Features

### Full-Text Search (FTS5)

**Built-in**, free, fast full-text search with advanced filters:

```bash
# Search entry text
jsearch "anxiety therapy"

# Combine text + metadata
jsearch "alice" person:alice in:2024

# Complex filtering
jsearch "reflection" city:montreal words:500-1000 has:manuscript
```

**How it works:**

- SQLite FTS5 virtual table with Porter stemming
- BM25 ranking for relevance scoring
- Searches actual entry content, not just metadata
- Auto-sync triggers keep index up-to-date

**Create search index:**

```bash
jsearch index --create
```

### Automated Text Analysis (Optional)

Progressive intelligence levels - use what you need:

#### Level 2: spaCy NER (Free) â­â­â­â­â˜†

**Entity extraction using ML:**

- Detects people, locations, cities, organizations, events
- Theme identification
- Confidence scoring

**Install:**

```bash
pip install spacy
python -m spacy download en_core_web_sm
```

**Usage:**

```bash
nlp analyze 2024-11-01 --level 2
```

#### Level 3: Sentence Transformers (Free) â­â­â­â­â˜†

**Semantic similarity search:**

- Find similar entries by meaning (not just keywords)
- Theme clustering
- Understanding context

**Install:**

```bash
pip install sentence-transformers
pip install faiss-cpu  # optional, for faster search
```

**Usage:**

```bash
# Find similar entries
nlp similar 2024-11-01 --limit 10

# Cluster by theme
nlp cluster --num-clusters 10
```

#### Level 4: LLM APIs (Paid) â­â­â­â­â­

**Most accurate analysis:**

- Advanced entity extraction
- Manuscript narrative analysis
- Character voice and arc suggestions
- Theme identification with context

**Two providers supported:**

**Claude (Anthropic):**

- Cost: ~$0.007/entry (Haiku), ~$0.075/entry (Sonnet)
- Install: `pip install anthropic`
- API Key: `export ANTHROPIC_API_KEY='your-key'`

**OpenAI (GPT-4):**

- Cost: ~$0.003/entry (GPT-4o mini), ~$0.025/entry (GPT-4o)
- Install: `pip install openai`
- API Key: `export OPENAI_API_KEY='your-key'`

**Usage:**

```bash
# Analyze with Claude (default)
nlp analyze 2024-11-01 --level 4 --manuscript

# Analyze with OpenAI
nlp analyze 2024-11-01 --level 4 --provider openai --manuscript

# Batch analyze with OpenAI
nlp batch --level 4 --provider openai --limit 10
```

**Check what's installed:**

```bash
nlp status
```

---

## Markdown Format

### Entry Structure

```yaml
---
date: 2024-01-15
word_count: 850
reading_time: 4.2

city: Montreal
locations: [Cafe X, Library]

people: [Maria-Jose, "@Friend (Full Name)"]
tags: [writing, reflection]
events: [thesis-defense]

epigraph: "Quote text here"
epigraph_attribution: Author Name

dates:
  - 2024-06-01 (thesis exam)

references:
  - content: "Referenced quote"
    speaker: Speaker Name
    source:
      title: Book Title
      type: book
      author: Author Name

manuscript:
  status: draft
  edited: false
  themes: [identity, memory]
---
## Monday, January 15, 2024

Entry content here...
```

See `example_yaml.md` for complete examples.

---

## Database Schema

Core tables:

- `entries` - Journal entries with metadata
- `people` - People mentioned (with aliases, relationships)
- `cities` - Geographic cities
- `locations` - Specific venues/places
- `events` - Thematic events spanning entries
- `tags` - Keyword tags
- `dates` - Referenced dates with context
- `references` - External citations
- `poems` - Poetry versions

Manuscript tables:

- `manuscript_entries` - Curation status
- `manuscript_people` - Character adaptations
- `manuscript_events` - Event transformations
- `themes` - Thematic categories
- `arcs` - Narrative arcs

---

## Development

### Running Tests

```bash
pytest tests/
```

### Continuous Integration

The project uses GitHub Actions for automated testing and quality checks:

**Test Workflow** (runs on every push/PR):

- Linting with Ruff
- Unit tests on Python 3.10, 3.11, 3.12
- Coverage reporting (80% minimum)

**Integration Workflow** (runs on main branch):

- Full integration tests with system dependencies
- Daily scheduled runs

**Security Workflow** (runs weekly):

- Dependency vulnerability scanning with pip-audit
- CodeQL security analysis

All workflows are defined in `.github/workflows/`. Status badges are displayed at the top of this README.

### Adding Migrations

```bash
# Create migration
metadb migration-create "add_new_field"

# Edit generated file in alembic/versions/

# Apply migration
metadb migration-upgrade
```

### Code Style

Uses Ruff for linting. Code follows:

- Type hints throughout
- Click for CLI interfaces
- Emoji-style terminal output
- Comprehensive logging

---

## Dependencies

### Core Dependencies

- Python 3.10+
- SQLAlchemy 2.0+
- Click 8.0+
- Pandoc 2.19+
- Tectonic or XeLaTeX
- Cormorant Garamond font (for PDFs)

See `environment.yaml` for complete list.

### Optional NLP Dependencies

**Level 2 (spaCy NER):**

```bash
pip install spacy
python -m spacy download en_core_web_sm
```

**Level 3 (Semantic Search):**

```bash
pip install sentence-transformers
pip install faiss-cpu  # optional, faster search
```

**Level 4 (LLM APIs):**

```bash
# Claude (Anthropic)
pip install anthropic
export ANTHROPIC_API_KEY='your-key'

# OpenAI (GPT-4)
pip install openai
export OPENAI_API_KEY='your-key'
```

---

## Configuration

Edit `dev/core/paths.py` to customize:

- Data directory location
- Database path
- Output directories
- Template paths

---

## About the Name

A _palimpsest_ is a manuscript that has been scraped clean for reuseâ€”yet traces of the original text remain visible beneath. This project embodies that concept: writing layered over writing, memory overwritten but never fully erased.

---

## License

MIT License - See LICENSE file for details.

---

## Acknowledgments

Built for managing journal archives from [750words.com](https://750words.com). Inspired by the practice of daily writing and the challenge of transforming private reflection into public art.
