#!/usr/bin/env python3
"""
export_manager.py
-----------------
Data export functionality for the database.
"""
import csv
import json
import shutil
from pathlib import Path
from datetime import datetime, timezone, date
from typing import Dict, Any, Union, Optional, List, Callable

from sqlalchemy.orm import Session

from dev.core.exceptions import ExportError
from dev.core.logging_manager import PalimpsestLogger
from dev.core.temporal_files import TemporalFileManager

from .decorators import handle_db_errors, log_database_operation
from .query_analytics import QueryAnalytics
from .query_optimizer import QueryOptimizer, HierarchicalBatcher, DateBatch

# Import models for export
from .models import (
    Entry,
    Person,
    Location,
    Event,
    Tag,
    Reference,
    ReferenceSource,
    MentionedDate,
    Poem,
    PoemVersion,
    Alias,
)
from .models_manuscript import (
    ManuscriptEntry,
    ManuscriptPerson,
    ManuscriptEvent,
    Theme,
    Arc,
)


class ExportManager:
    """
    Handles data export operations for the database.

    Provides CSV and JSON export functionality with temporal file management
    for safe export operations.
    """

    def __init__(self, logger: Optional[PalimpsestLogger] = None) -> None:
        """
        Initialize export manager.

        Args:
            logger: Optional logger for export operations
        """
        self.logger = logger

    @handle_db_errors
    @log_database_operation("export_entries_optimized")
    def export_entries_optimized(
        self,
        session: Session,
        entry_ids: List[int],
        export_callback: Callable,
        **callback_kwargs,
    ) -> Dict[str, Any]:
        """
        Export entries using optimized loading.

        This is a helper method for other export operations. It loads
        entries with all relationships preloaded and calls the provided
        callback for each entry.

        Args:
            session: SQLAlchemy session
            entry_ids: List of entry IDs to export
            export_callback: Function to call for each entry
            **callback_kwargs: Additional arguments for callback

        Returns:
            Dictionary with export statistics

        Examples:
            >>> def my_export(entry, output_dir):
            ...     # Do something with entry
            ...     pass
            >>>
            >>> stats = export_manager.export_entries_optimized(
            ...     session,
            ...     entry_ids,
            ...     my_export,
            ...     output_dir="/path/to/output"
            ... )
        """
        stats = {
            "total": len(entry_ids),
            "processed": 0,
            "errors": 0,
            "start_time": datetime.now(),
        }

        # Use optimized loading internally
        entries = QueryOptimizer.for_export(session, entry_ids)

        for entry in entries:
            try:
                export_callback(entry, **callback_kwargs)
                stats["processed"] += 1
            except Exception as e:
                stats["errors"] += 1
                if self.logger:
                    self.logger.log_error(
                        e, {"operation": "export_entry", "entry_id": entry.id}
                    )

        stats["duration"] = (datetime.now() - stats["start_time"]).total_seconds()
        return stats

    @handle_db_errors
    @log_database_operation("export_hierarchical")
    def export_hierarchical(
        self,
        session: Session,
        export_callback: Callable,
        threshold: int = 500,
        **callback_kwargs,
    ) -> Dict[str, Any]:
        """
        Export all entries using hierarchical batching for optimal performance.

        Automatically batches entries by year or month based on volume,
        using optimized relationship loading for each batch.

        Args:
            session: SQLAlchemy session
            export_callback: Function to call for each entry
            threshold: Batch size threshold (default: 500)
            **callback_kwargs: Additional arguments for callback

        Returns:
            Dictionary with export statistics

        Examples:
            >>> def export_to_file(entry, output_dir):
            ...     # Export entry to file
            ...     pass
            >>>
            >>> stats = export_manager.export_hierarchical(
            ...     session,
            ...     export_to_file,
            ...     threshold=500,
            ...     output_dir="/path/to/output"
            ... )
        """
        stats = {
            "total_batches": 0,
            "total_entries": 0,
            "processed": 0,
            "errors": 0,
            "batches": [],
            "start_time": datetime.now(),
        }

        # Use HierarchicalBatcher internally
        batches = HierarchicalBatcher.create_batches(session, threshold)
        stats["total_batches"] = len(batches)

        for batch in batches:
            batch_stats = {
                "period": batch.period_label,
                "entries": batch.entry_count,
                "processed": 0,
                "errors": 0,
            }

            stats["total_entries"] += batch.entry_count

            # All relationships already preloaded by QueryOptimizer!
            for entry in batch.entries:
                try:
                    export_callback(entry, **callback_kwargs)
                    batch_stats["processed"] += 1
                    stats["processed"] += 1
                except Exception as e:
                    batch_stats["errors"] += 1
                    stats["errors"] += 1
                    if self.logger:
                        self.logger.log_error(
                            e, {"operation": "export_entry", "entry_id": entry.id}
                        )

            stats["batches"].append(batch_stats)

            if self.logger:
                self.logger.log_operation(
                    "batch_exported",
                    {
                        "period": batch.period_label,
                        "processed": batch_stats["processed"],
                        "errors": batch_stats["errors"],
                    },
                )

        stats["duration"] = (datetime.now() - stats["start_time"]).total_seconds()
        return stats

    @handle_db_errors
    @log_database_operation("get_export_batches")
    def get_export_batches(
        self, session: Session, threshold: int = 500
    ) -> List[DateBatch]:
        """
        Get hierarchical batches for custom export operations.

        Use this when you need more control over the export process
        but still want optimized batching.

        Args:
            session: SQLAlchemy session
            threshold: Batch size threshold (default: 500)

        Returns:
            List of DateBatch objects with preloaded entries

        Examples:
            >>> batches = export_manager.get_export_batches(session)
            >>> for batch in batches:
            ...     print(f"Processing {batch.period_label}")
            ...     for entry in batch.entries:
            ...         # All relationships already loaded
            ...         process_entry(entry)
        """
        return HierarchicalBatcher.create_batches(session, threshold)

    @handle_db_errors
    @log_database_operation("export_to_csv")
    def export_to_csv(
        self, session: Session, export_dir: Union[str, Path]
    ) -> Dict[str, Path]:
        """
        Export all database tables to CSV files.

        Args:
            session: SQLAlchemy session
            export_dir: Directory for CSV files

        Returns:
            Dictionary mapping table names to exported file paths
        """
        export_dir = Path(export_dir)
        export_dir.mkdir(parents=True, exist_ok=True)

        exported_files = {}

        # Define tables to export
        tables = {
            "entries": Entry,
            "people": Person,
            "locations": Location,
            "events": Event,
            "tags": Tag,
            "references": Reference,
            "reference_sources": ReferenceSource,
            "mentioned_dates": MentionedDate,
            "poems": Poem,
            "poem_versions": PoemVersion,
            "aliases": Alias,
            "manuscript_entries": ManuscriptEntry,
            "manuscript_people": ManuscriptPerson,
            "manuscript_events": ManuscriptEvent,
            "themes": Theme,
            "arcs": Arc,
        }

        with TemporalFileManager() as temp_manager:
            for table_name, model_class in tables.items():
                try:
                    exported_files[table_name] = self._export_table_to_csv(
                        session, model_class, table_name, export_dir, temp_manager
                    )
                except ExportError as e:
                    if self.logger:
                        self.logger.log_error(
                            e, {"operation": "export_table_csv", "table": table_name}
                        )
                    raise  # Re-raise instead of continuing
                except Exception as e:
                    if self.logger:
                        self.logger.log_error(
                            e, {"operation": "export_table_csv", "table": table_name}
                        )
                    raise ExportError(f"Failed to export table {table_name}: {e}")

        return exported_files

    def _export_table_to_csv(
        self,
        session: Session,
        model_class: type,
        table_name: str,
        export_dir: Path,
        temp_manager: TemporalFileManager,
    ) -> Path:
        """
        Export a single table to CSV.

        Args:
            session: SQLAlchemy session
            model_class: SQLAlchemy model class
            table_name: Name of the table
            export_dir: Export directory
            temp_manager: Temporal file manager

        Returns:
            Path to exported CSV file
        """
        temp_file = temp_manager.create_temp_file(suffix=".csv")
        final_file = export_dir / f"{table_name}.csv"

        with open(temp_file, "w", newline="", encoding="utf-8") as csvfile:
            records = session.query(model_class).all()

            if records:
                # Get column names from the first record
                fieldnames = [column.name for column in model_class.__table__.columns]

                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                for record in records:
                    row_data = {}
                    for field in fieldnames:
                        value = getattr(record, field, None)
                        if isinstance(value, (datetime, date)):
                            value = value.isoformat()
                        elif value is not None and hasattr(value, "value"):  # Enum
                            value = value.value
                        row_data[field] = value
                    writer.writerow(row_data)

        # Move temp file to final location
        shutil.move(str(temp_file), str(final_file))
        return final_file

    @handle_db_errors
    @log_database_operation("export_to_json")
    def export_to_json(
        self,
        session: Session,
        export_file: Union[str, Path],
        batch_size: int = 100,
    ) -> Path:
        """
        Export complete database to JSON.

        Args:
            session: SQLAlchemy session
            export_file: Path for JSON export file
            batch_size: Number of entries to process at once

        Returns:
            Path to exported JSON file
        """
        export_file = Path(export_file)

        with TemporalFileManager() as temp_manager:
            temp_file = temp_manager.create_temp_file(suffix=".json")

            with open(temp_file, "w", encoding="utf-8") as f:
                f.write("{\n")
                f.write(
                    f'  "export_timestamp": "{datetime.now(timezone.utc).isoformat()}",\n'
                )
                # Get stats (lightweight)
                analytics = QueryAnalytics(self.logger)
                stats = analytics.get_database_stats(session)
                f.write(f'  "database_stats": {json.dumps(stats, default=str)},\n')

                # Stream entries in batches
                f.write('  "entries": [\n')

                total_entries = session.query(Entry).count()
                offset = 0
                first_entry = True

                while offset < total_entries:
                    entry_ids = (
                        session.query(Entry.id)
                        .order_by(Entry.date)
                        .limit(batch_size)
                        .offset(offset)
                        .all()
                    )

                    entry_ids = [e_id for (e_id,) in entry_ids]

                    # Preload all relationships at once
                    entries = QueryOptimizer.for_export(session, entry_ids)

                    for entry in entries:
                        if not first_entry:
                            f.write(",\n")
                        first_entry = False

                        entry_data = self._serialize_entry(entry)
                        f.write("    ")
                        f.write(json.dumps(entry_data, ensure_ascii=False, default=str))

                    offset += batch_size
                    session.expunge_all()  # Clear session to free memory

                f.write("\n  ],\n")

                # Export other tables (people, locations, events) in batches
                # TODO: Add missing tables here
                f.write('  "people": [\n')
                people = session.query(Person).all()
                for i, person in enumerate(people):
                    if i > 0:
                        f.write(",\n")
                    person_data = self._serialize_person(person)
                    f.write("    ")
                    f.write(json.dumps(person_data, ensure_ascii=False, default=str))
                f.write("\n  ],\n")

                # Locations
                f.write('  "locations": [\n')
                locations = session.query(Location).all()
                for i, location in enumerate(locations):
                    if i > 0:
                        f.write(",\n")
                    f.write("    ")
                    f.write(
                        json.dumps(
                            {
                                "id": location.id,
                                "name": location.name,
                            },
                            ensure_ascii=False,
                            default=str,
                        )
                    )
                f.write("\n  ],\n")

                # Events
                f.write('  "events": [\n')
                events = session.query(Event).all()
                for i, event in enumerate(events):
                    if i > 0:
                        f.write(",\n")
                    f.write("    ")
                    f.write(
                        json.dumps(
                            {
                                "id": event.id,
                                "event": event.event,
                                "title": event.title,
                                "description": event.description,
                            },
                            ensure_ascii=False,
                            default=str,
                        )
                    )
                f.write("\n  ]\n")

                # Close JSON
                f.write("}\n")

            # Move to final location
            export_file.parent.mkdir(parents=True, exist_ok=True)
            shutil.move(str(temp_file), str(export_file))

        return export_file

    def _serialize_entry(self, entry: Entry) -> Dict[str, Any]:
        """
        Serialize an entry with all its relationships.

        Args:
            entry: Entry instance

        Returns:
            Dictionary with serialized entry data
        """
        return {
            "id": entry.id,
            "date": entry.date.isoformat() if entry.date else None,
            "file_path": entry.file_path,
            "file_hash": entry.file_hash,
            "word_count": entry.word_count,
            "reading_time": entry.reading_time,
            "epigraph": entry.epigraph,
            "epigraph_attribution": entry.epigraph_attribution,
            "notes": entry.notes,
            "created_at": entry.created_at.isoformat() if entry.created_at else None,
            "updated_at": entry.updated_at.isoformat() if entry.updated_at else None,
            # Relationships
            "mentioned_dates": [
                {"date": md.date.isoformat(), "context": md.context}
                for md in entry.dates
            ],
            "cities": [city.city for city in entry.cities],
            "locations": [
                {"name": loc.name, "city": loc.city.city if loc.city else None}
                for loc in entry.locations
            ],
            "people": [person.display_name for person in entry.people],
            "events": [event.display_name for event in entry.events],
            "tags": [tag.tag for tag in entry.tags],
            "related_entries": [re.date.isoformat() for re in entry.related_entries],
            # References
            "references": [
                {
                    "content": ref.content if ref.content else None,
                    "description": ref.description if ref.description else None,
                    "mode": ref.mode.value if ref.mode else "direct",
                    "speaker": ref.speaker,
                    "source": (
                        {
                            "title": ref.source.title if ref.source else None,
                            "type": ref.source.type if ref.source else None,
                            "author": ref.source.author if ref.source else None,
                        }
                        if ref.source
                        else None
                    ),
                }
                for ref in entry.references
            ],
            # Poems
            "poems": [
                {
                    "title": pv.poem.title if pv.poem else None,
                    "content": pv.content,
                    "notes": pv.notes,
                    "revision_date": (
                        pv.revision_date.isoformat() if pv.revision_date else None
                    ),
                }
                for pv in entry.poems
            ],
            # Manuscript data
            "manuscript": (
                {
                    "status": entry.manuscript.status.value,
                    "edited": entry.manuscript.edited,
                    "notes": entry.manuscript.notes,
                    "themes": [theme.theme for theme in entry.manuscript.themes],
                }
                if entry.manuscript
                else None
            ),
        }

    def _serialize_person(self, person: Person) -> Dict[str, Any]:
        """
        Serialize a person with all their data.

        Args:
            person: Person instance

        Returns:
            Dictionary with serialized person data
        """
        return {
            "id": person.id,
            "name": person.name,
            "full_name": person.full_name,
            "name_fellow": person.name_fellow,
            "relation_type": (
                person.relation_type.value if person.relation_type else None
            ),
            "aliases": [alias.alias for alias in person.aliases],
            "entry_count": person.entry_count,
            "first_appearance": (
                person.first_appearance.isoformat() if person.first_appearance else None
            ),
            "last_appearance": (
                person.last_appearance.isoformat() if person.last_appearance else None
            ),
            "manuscript": (
                {"character": person.manuscript.character}
                if person.manuscript
                else None
            ),
        }
